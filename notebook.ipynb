{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 2021 NMA Project Team",
   "metadata": {
    "tags": [],
    "cell_id": "00000-d9f2fac3-84fd-4d60-9f79-aa5d993afb3d",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Anjali Srinivasan, Aaditya Prasad, Zac Wheeler",
   "metadata": {
    "tags": [],
    "cell_id": "00001-cc9cfd30-b852-4361-a70c-e92369eda8c4",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-6be5eadb-c8b6-4f2e-8b2e-d71d54803269",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "293a735e",
    "execution_start": 1626191326846,
    "execution_millis": 11291,
    "deepnote_cell_type": "code"
   },
   "source": "# imports\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for visualization\n!pip install nilearn --quiet\nfrom nilearn import plotting, datasets",
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n/root/venv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n  \"Numpy arrays.\", FutureWarning)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## Data Retrieval",
   "metadata": {
    "tags": [],
    "cell_id": "00003-a5fc6747-b055-4c04-a3a2-82a16186524b",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-95c2e06e-3fd8-4533-bc17-36411e27b5d6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f620f31e",
    "execution_start": 1626191338139,
    "execution_millis": 12,
    "deepnote_cell_type": "code"
   },
   "source": "# The download cells will store the data in nested directories starting here:\nHCP_DIR = \"./hcp\"\nif not os.path.isdir(HCP_DIR):\n  os.mkdir(HCP_DIR)\n\n# The data shared for NMA projects is a subset of the full HCP dataset\nN_SUBJECTS = 339\n\n# The data have already been aggregated into ROIs from the Glasesr parcellation\nN_PARCELS = 360\n\n# The acquisition parameters for all tasks were identical\nTR = 0.72  # Time resolution, in sec\n\n# The parcels are matched across hemispheres with the same order\nHEMIS = [\"Right\", \"Left\"]\n\n# Each experiment was repeated multiple times in each subject\nN_RUNS_REST = 4\nN_RUNS_TASK = 2\n\n# Time series data are organized by experiment, with each experiment\n# having an LR and RL (phase-encode direction) acquistion\nBOLD_NAMES = [\n  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n]\n\n# You may want to limit the subjects used during code development.\n# This will use all subjects:\nsubjects = range(N_SUBJECTS)",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "### Data Download",
   "metadata": {
    "tags": [],
    "cell_id": "00005-363b67db-270e-455b-a563-9568c1929e2d",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-6cbac226-42f5-41b4-9645-81b7f6ef5473",
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_task.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/s4h8j/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-db894fff-d035-4400-a70d-8d3c23b54e05",
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_covariates.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/x5p4g/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-9717b64f-e5b6-4c29-849e-435481bf1ca0",
    "deepnote_cell_type": "code"
   },
   "source": "fname = f\"{HCP_DIR}/atlas.npz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/j5kuc/download",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Region Info",
   "metadata": {
    "tags": [],
    "cell_id": "00009-9891b871-8383-49c3-a134-d279f618a532",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-51057d4a-022c-49a0-b1a2-3be653b5a8c4",
    "deepnote_cell_type": "code"
   },
   "source": "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\nregion_info = dict(\n    name=regions[0].tolist(),\n    network=regions[1],\n    myelin=regions[2].astype(np.float),\n)\n\n#print(len(region_info[\"name\"]))\n#print(len(region_info[\"network\"]))\n#print(len(region_info[\"myelin\"]))\n\n#print(region_info[\"name\"])\n#print(region_info[\"network\"])\n#print()\n\n#names = region_info[\"name\"]\n#networks = region_info[\"network\"]\n\n#language_areas = []\n\n#for i in range(len(networks)):\n#  if networks[i] == \"Auditory\":\n#    print(i, names[i])\n\n#posterior-mu = posterior multimodal",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-dd9e770d-23e3-40bc-84b0-0bd6ec86f935",
    "deepnote_cell_type": "code"
   },
   "source": "with np.load(f\"{HCP_DIR}/atlas.npz\") as dobj:\n  atlas = dict(**dobj)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Helper Functions (For Data Loading)",
   "metadata": {
    "tags": [],
    "cell_id": "00012-1f89d1c6-ce57-4b7a-95ae-ebdc7b8c5d4b",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "get_image_ids(name)",
   "metadata": {
    "tags": [],
    "cell_id": "00013-f6c65a17-0641-4899-94c5-c52c6c9b69c8",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00013-63f39e62-8dbc-4787-93f7-d5f38d1e7337",
    "deepnote_cell_type": "code"
   },
   "source": "def get_image_ids(name):\n  \"\"\"Get the 1-based image indices for runs in a given experiment.\n\n    Args:\n      name (str) : Name of experiment (\"rest\" or name of task) to load\n    Returns:\n      run_ids (list of int) : Numeric ID for experiment image files\n\n  \"\"\"\n  run_ids = [\n    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n  ]\n  if not run_ids:\n    raise ValueError(f\"Found no data for '{name}''\")\n  return run_ids",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "load_timeseries(subject, name, runs=None, concat=True, remove_mean=True)",
   "metadata": {
    "tags": [],
    "cell_id": "00015-3f8dbcb9-e6cf-4390-92c8-42833354c0c3",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-88664f75-e086-4cf3-9b66-df2832929174",
    "deepnote_cell_type": "code"
   },
   "source": "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of experiment (\"rest\" or name of task) to load\n    run (None or int or list of ints): 0-based run(s) of the task to load,\n      or None to load all runs.\n    concat (bool) : If True, concatenate multiple runs in time\n    remove_mean (bool) : If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_tp array): Array of BOLD data values\n\n  \"\"\"\n  # Get the list relative 0-based index of runs to use\n  if runs is None:\n    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n  elif isinstance(runs, int):\n    runs = [runs]\n\n  # Get the first (1-based) run id for this experiment \n  offset = get_image_ids(name)[0]\n\n  # Load each run's data\n  bold_data = [\n      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n  ]\n\n  # Optionally concatenate in time\n  if concat:\n    bold_data = np.concatenate(bold_data, axis=-1)\n\n  return bold_data\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "load_single_timeseries(subject, bold_run, remove_mean=True)",
   "metadata": {
    "tags": [],
    "cell_id": "00017-9b6c5c22-d325-48ad-8ddd-289be657b20b",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-ab7ac8ec-32c4-45dd-b148-33028011d330",
    "deepnote_cell_type": "code"
   },
   "source": "def load_single_timeseries(subject, bold_run, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject and single run.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    bold_run (int): 1-based run index, across all tasks\n    remove_mean (bool): If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_timepoint array): Array of BOLD data values\n\n  \"\"\"\n  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n  ts = np.load(f\"{bold_path}/{bold_file}\")\n  if remove_mean:\n    ts -= ts.mean(axis=1, keepdims=True)\n  return ts",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "load_evs(subject, name, condition)",
   "metadata": {
    "tags": [],
    "cell_id": "00019-1d26dca3-0b1c-417c-92b5-b06c0551dc72",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-df404562-041f-4b69-aea2-1b83cfc5fd95",
    "deepnote_cell_type": "code"
   },
   "source": "def load_evs(subject, name, condition):\n  \"\"\"Load EV (explanatory variable) data for one task condition.\n\n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of task\n    condition (str) : Name of condition\n\n  Returns\n    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n      of the condition for each run.\n\n  \"\"\"\n  evs = []\n  for id in get_image_ids(name):\n    task_key = BOLD_NAMES[id - 1]\n    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n    evs.append(ev)\n  return evs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "condition_frames(run_evs, skip=0)",
   "metadata": {
    "tags": [],
    "cell_id": "00021-402a190f-d2c9-49f5-92b5-7740b71a3f31",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "def condition_frames(run_evs, skip=0):\n  \"\"\"Identify timepoints corresponding to a given condition in each run.\n\n  Args:\n    run_evs (list of dicts) : Onset and duration of the event, per run\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:\n    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n\n  \"\"\"\n  frames_list = []\n  for ev in run_evs:\n\n    # Determine when trial starts, rounded down\n    start = np.floor(ev[\"onset\"] / TR).astype(int)\n\n    # Use trial duration to determine how many frames to include for trial\n    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n\n    # Take the range of frames that correspond to this specific trial\n    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n\n    frames_list.append(np.concatenate(frames))\n\n  return frames_list",
   "metadata": {
    "tags": [],
    "cell_id": "00021-651ebe8f-f476-46bd-a7c1-d4b59222a85c",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "selective_average(timeseries_data, ev, skip=0)",
   "metadata": {
    "tags": [],
    "cell_id": "00023-1054ac49-b400-4ca3-8fc9-efd8d4070e5a",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "def selective_average(timeseries_data, ev, skip=0):\n  \"\"\"Take the temporal mean across frames for a given condition.\n\n  Args:\n    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n    ev (dict or list of dicts): Condition timing information\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:\n    avg_data (1D array): Data averagted across selected image frames based\n    on condition timing\n\n  \"\"\"\n  # Ensure that we have lists of the same length\n  if not isinstance(timeseries_data, list):\n    timeseries_data = [timeseries_data]\n  if not isinstance(ev, list):\n    ev = [ev]\n  if len(timeseries_data) != len(ev):\n    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n\n  # Identify the indices of relevant frames\n  frames = condition_frames(ev, skip)\n\n  # Select the frames from each image\n  selected_data = []\n  for run_data, run_frames in zip(timeseries_data, frames):\n    run_frames = run_frames[run_frames < run_data.shape[1]]\n    selected_data.append(run_data[:, run_frames])\n\n  # Take the average in each parcel\n  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n\n  return avg_data",
   "metadata": {
    "tags": [],
    "cell_id": "00023-4bd3e35b-22c5-4c16-8795-b5c13629cdfd",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Task Analysis",
   "metadata": {
    "tags": [],
    "cell_id": "00025-43aaa8a0-dab4-4bb3-a2fa-f3ca7e4fc7fa",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Task Descriptions",
   "metadata": {
    "tags": [],
    "cell_id": "00026-c3c731db-05cb-4fcf-8752-adfd9a3c1c2f",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "MOTOR:&nbsp;cue,&nbsp;lf,&nbsp;lh,&nbsp;rf,&nbsp;rh,&nbsp;t",
   "metadata": {
    "tags": [],
    "cell_id": "00027-34935fad-ac74-4fe4-857c-0b76d7d23138",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "WM:&nbsp;&nbsp;&nbsp;&nbsp;0bk_body,&nbsp;0bk_faces,&nbsp;0bk_nir,&nbsp;0bk_placed,&nbsp;0bk_tools,&nbsp;&nbsp;2bk_body,&nbsp;2bk_faces,&nbsp;2bk_nir,&nbsp;2bk_placed,&nbsp;2bk_tools,&nbsp;0bk_cor,&nbsp;0bk_err,&nbsp;&nbsp;2bk_cor,&nbsp;2bk_err,&nbsp;&nbsp;all_bk_cor,&nbsp;all_bk_err",
   "metadata": {
    "tags": [],
    "cell_id": "00028-1fb60741-5370-4c54-9bd3-f100155f0be1",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "EMOTION:&nbsp;feat,&nbsp;neutral",
   "metadata": {
    "tags": [],
    "cell_id": "00029-c4e56c9a-bc09-472c-9b56-0630de621780",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "GAMBLING:&nbsp;loss,&nbsp;loss_event,&nbsp;win,&nbsp;win_event,&nbsp;neut_event",
   "metadata": {
    "tags": [],
    "cell_id": "00030-6ba83491-909a-4dfc-80a0-20d638d21250",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "LANGUAGE:&nbsp;cue, math,&nbsp;story, present_math,&nbsp;present_story,&nbsp;question_math,&nbsp;question_story,&nbsp;response_math,&nbsp;response_story",
   "metadata": {
    "tags": [],
    "cell_id": "00031-6b6decc4-7058-4caf-a16c-0a5ac2a4cf45",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "RELATIONAL:&nbsp;error,&nbsp;match,&nbsp;relation",
   "metadata": {
    "tags": [],
    "cell_id": "00032-9d385d85-c308-43b8-8a96-002791b220a1",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "SOCIAL:&nbsp;mental_resp,&nbsp;mental,&nbsp;other_resp,&nbsp;rnd",
   "metadata": {
    "tags": [],
    "cell_id": "00033-3f400221-919b-49c1-8d42-ab5d7ca043a4",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0cfc2388-b22c-424d-b321-d09bf45f57a9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "c94308f2-8344-4770-9a29-afd426ed4526",
  "deepnote_execution_queue": []
 }
}