{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 2021 NMA Project Team",
   "metadata": {
    "tags": [],
    "cell_id": "00000-d9f2fac3-84fd-4d60-9f79-aa5d993afb3d",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Anjali Srinivasan, Aaditya Prasad, Zac Wheeler",
   "metadata": {
    "tags": [],
    "cell_id": "00001-cc9cfd30-b852-4361-a70c-e92369eda8c4",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-6be5eadb-c8b6-4f2e-8b2e-d71d54803269",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a90608b6",
    "execution_start": 1626705152468,
    "execution_millis": 11550,
    "deepnote_cell_type": "code"
   },
   "source": "# imports\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# for visualization\n!pip install nilearn --quiet\nfrom nilearn import plotting, datasets",
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n/root/venv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n  \"Numpy arrays.\", FutureWarning)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Retrieval",
   "metadata": {
    "tags": [],
    "cell_id": "00003-a5fc6747-b055-4c04-a3a2-82a16186524b",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-95c2e06e-3fd8-4533-bc17-36411e27b5d6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f620f31e",
    "execution_start": 1626705164042,
    "execution_millis": 11,
    "deepnote_cell_type": "code"
   },
   "source": "# The download cells will store the data in nested directories starting here:\nHCP_DIR = \"./hcp\"\nif not os.path.isdir(HCP_DIR):\n  os.mkdir(HCP_DIR)\n\n# The data shared for NMA projects is a subset of the full HCP dataset\nN_SUBJECTS = 339\n\n# The data have already been aggregated into ROIs from the Glasesr parcellation\nN_PARCELS = 360\n\n# The acquisition parameters for all tasks were identical\nTR = 0.72  # Time resolution, in sec\n\n# The parcels are matched across hemispheres with the same order\nHEMIS = [\"Right\", \"Left\"]\n\n# Each experiment was repeated multiple times in each subject\nN_RUNS_REST = 4\nN_RUNS_TASK = 2\n\n# Time series data are organized by experiment, with each experiment\n# having an LR and RL (phase-encode direction) acquistion\nBOLD_NAMES = [\n  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n]\n\n# You may want to limit the subjects used during code development.\n# This will use all subjects:\nsubjects = range(N_SUBJECTS)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Download",
   "metadata": {
    "tags": [],
    "cell_id": "00005-363b67db-270e-455b-a563-9568c1929e2d",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-6cbac226-42f5-41b4-9645-81b7f6ef5473",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "76c45edd",
    "execution_start": 1626705164078,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_task.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/s4h8j/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-db894fff-d035-4400-a70d-8d3c23b54e05",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c2c5c0bd",
    "execution_start": 1626705164105,
    "execution_millis": 23,
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_covariates.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/x5p4g/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-9717b64f-e5b6-4c29-849e-435481bf1ca0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "81d454e4",
    "execution_start": 1626705164183,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "fname = f\"{HCP_DIR}/atlas.npz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/j5kuc/download",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Region Info",
   "metadata": {
    "tags": [],
    "cell_id": "00009-9891b871-8383-49c3-a134-d279f618a532",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-51057d4a-022c-49a0-b1a2-3be653b5a8c4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "24a25f8d",
    "execution_start": 1626705164184,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\nregion_info = dict(\n    name=regions[0].tolist(),\n    network=regions[1],\n    myelin=regions[2].astype(np.float),\n)\n\n#print(len(region_info[\"name\"]))\n#print(len(region_info[\"network\"]))\n#print(len(region_info[\"myelin\"]))\n\n#print(region_info[\"name\"])\n#print(region_info[\"network\"])\n#print()\n\n#names = region_info[\"name\"]\n#networks = region_info[\"network\"]\n\n#language_areas = []\n\n#for i in range(len(networks)):\n#  if networks[i] == \"Auditory\":\n#    print(i, names[i])\n\n#posterior-mu = posterior multimodal",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-dd9e770d-23e3-40bc-84b0-0bd6ec86f935",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "68c98641",
    "execution_start": 1626705164224,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "with np.load(f\"{HCP_DIR}/atlas.npz\") as dobj:\n  atlas = dict(**dobj)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Helper Functions (For Data Loading)",
   "metadata": {
    "tags": [],
    "cell_id": "00012-1f89d1c6-ce57-4b7a-95ae-ebdc7b8c5d4b",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "get_image_ids(name)",
   "metadata": {
    "tags": [],
    "cell_id": "00013-f6c65a17-0641-4899-94c5-c52c6c9b69c8",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00013-63f39e62-8dbc-4787-93f7-d5f38d1e7337",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6be28f60",
    "execution_start": 1626705164225,
    "execution_millis": 81254,
    "deepnote_cell_type": "code"
   },
   "source": "def get_image_ids(name):\n  \"\"\"Get the 1-based image indices for runs in a given experiment.\n\n    Args:\n      name (str) : Name of experiment (\"rest\" or name of task) to load\n    Returns:\n      run_ids (list of int) : Numeric ID for experiment image files\n\n  \"\"\"\n  run_ids = [\n    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n  ]\n  if not run_ids:\n    raise ValueError(f\"Found no data for '{name}''\")\n  return run_ids",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_timeseries(subject, name, runs=None, concat=True, remove_mean=True)",
   "metadata": {
    "tags": [],
    "cell_id": "00015-3f8dbcb9-e6cf-4390-92c8-42833354c0c3",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-88664f75-e086-4cf3-9b66-df2832929174",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2d7b457b",
    "execution_start": 1626705164283,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of experiment (\"rest\" or name of task) to load\n    run (None or int or list of ints): 0-based run(s) of the task to load,\n      or None to load all runs.\n    concat (bool) : If True, concatenate multiple runs in time\n    remove_mean (bool) : If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_tp array): Array of BOLD data values\n\n  \"\"\"\n  # Get the list relative 0-based index of runs to use\n  if runs is None:\n    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n  elif isinstance(runs, int):\n    runs = [runs]\n\n  # Get the first (1-based) run id for this experiment \n  offset = get_image_ids(name)[0]\n\n  # Load each run's data\n  bold_data = [\n      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n  ]\n\n  # Optionally concatenate in time\n  if concat:\n    bold_data = np.concatenate(bold_data, axis=-1)\n\n  return bold_data\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_single_timeseries(subject, bold_run, remove_mean=True)",
   "metadata": {
    "tags": [],
    "cell_id": "00017-9b6c5c22-d325-48ad-8ddd-289be657b20b",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-ab7ac8ec-32c4-45dd-b148-33028011d330",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d3045ff1",
    "execution_start": 1626705164284,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def load_single_timeseries(subject, bold_run, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject and single run.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    bold_run (int): 1-based run index, across all tasks\n    remove_mean (bool): If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_timepoint array): Array of BOLD data values\n\n  \"\"\"\n  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n  ts = np.load(f\"{bold_path}/{bold_file}\")\n  if remove_mean:\n    ts -= ts.mean(axis=1, keepdims=True)\n  return ts",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_evs(subject, name, condition)",
   "metadata": {
    "tags": [],
    "cell_id": "00019-1d26dca3-0b1c-417c-92b5-b06c0551dc72",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-df404562-041f-4b69-aea2-1b83cfc5fd95",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c0d0c9",
    "execution_start": 1626705164324,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def load_evs(subject, name, condition):\n  \"\"\"Load EV (explanatory variable) data for one task condition.\n\n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of task\n    condition (str) : Name of condition\n\n  Returns\n    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n      of the condition for each run.\n\n  \"\"\"\n  evs = []\n  for id in get_image_ids(name):\n    task_key = BOLD_NAMES[id - 1]\n    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n    evs.append(ev)\n  return evs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "condition_frames(run_evs, skip=0)",
   "metadata": {
    "tags": [],
    "cell_id": "00021-402a190f-d2c9-49f5-92b5-7740b71a3f31",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00021-651ebe8f-f476-46bd-a7c1-d4b59222a85c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c744e3d7",
    "execution_start": 1626705164325,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "def condition_frames(run_evs, skip=0):\n  \"\"\"Identify timepoints corresponding to a given condition in each run.\n\n  Args:\n    run_evs (list of dicts) : Onset and duration of the event, per run\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:\n    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n\n  \"\"\"\n  frames_list = []\n  for ev in run_evs:\n\n    # Determine when trial starts, rounded down\n    start = np.floor(ev[\"onset\"] / TR).astype(int)\n\n    # Use trial duration to determine how many frames to include for trial\n    # TR = 0.72  # Time resolution, in sec\n    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n\n    # Take the range of frames that correspond to this specific trial\n    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n\n    frames_list.append(np.concatenate(frames))\n\n  return frames_list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "selective_average(timeseries_data, ev, skip=0)",
   "metadata": {
    "tags": [],
    "cell_id": "00023-1054ac49-b400-4ca3-8fc9-efd8d4070e5a",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00023-4bd3e35b-22c5-4c16-8795-b5c13629cdfd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1d5945ad",
    "execution_start": 1626705164388,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def selective_average(timeseries_data, ev, skip=0):\n  \"\"\"Take the temporal mean across frames for a given condition.\n\n  Args:\n    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n    ev (dict or list of dicts): Condition timing information\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:\n    avg_data (1D array): Data averaged across selected image frames based\n    on condition timing\n\n  \"\"\"\n  # Ensure that we have lists of the same length\n  if not isinstance(timeseries_data, list):\n    timeseries_data = [timeseries_data]\n  if not isinstance(ev, list):\n    ev = [ev]\n  if len(timeseries_data) != len(ev):\n    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n\n  # Identify the indices of relevant frames\n  frames = condition_frames(ev, skip)\n  print(\"frames shape: \", end=\": \")\n  print(np.array(frames).shape)\n  #print(frames)\n\n  # Select the frames from each image\n  selected_data = np.zeros(9)\n  for run_data, run_frames in zip(timeseries_data, frames):\n    #print(\"run_frames before: \", end=\": \")\n    #print(run_frames)\n\n    run_frames = run_frames[run_frames < run_data.shape[1]]\n    #separated_data = np.array([])\n    differences = np.diff(run_frames)\n\n    print(\"differences:\", end=\" \")\n    print(differences)\n\n\n    #separate out trials!!! \n\n    print(\"run_frames after: \", end=\": \")\n    print(run_frames)\n    \n    selected_data.append(run_data[:, run_frames])\n\n  # Take the average in each parcel\n\n  # right now: taking all data in timeseries for condition, and all trial data\n  # for that condition, and averaging EVERYTHING :)\n  # avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n\n  #print(np.array(selected_data, dtype=object).shape)\n  #print(a)\n  return selected_data",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Task Analysis",
   "metadata": {
    "tags": [],
    "cell_id": "00025-43aaa8a0-dab4-4bb3-a2fa-f3ca7e4fc7fa",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### timeseries_task",
   "metadata": {
    "tags": [],
    "cell_id": "00026-6642f39b-2187-4fbb-b0ab-dcd526e17150",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00026-64aa145e-a4ed-446b-b5fa-f770257984da",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "755868b1",
    "execution_start": 1626705164432,
    "execution_millis": 2621,
    "deepnote_cell_type": "code"
   },
   "source": "timeseries_task = []\n\n# timeseries_task format: subject, then LR/RL, then parcel, then time\n# 339, 2, 360, 316\nfor subject in subjects:\n  timeseries_task.append(load_timeseries(subject, \"language\", concat=False))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-dc209164-da58-4956-93e3-7d2d1fee6a20",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f944442a",
    "execution_start": 1626705167069,
    "execution_millis": 12,
    "deepnote_cell_type": "code"
   },
   "source": "#print(np.array(timeseries_task).shape)\n#print(timeseries_task[0])\n#print()\n#print(timeseries_task[0][0][0][0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Task Descriptions",
   "metadata": {
    "tags": [],
    "cell_id": "00026-c3c731db-05cb-4fcf-8752-adfd9a3c1c2f",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "MOTOR:&nbsp;cue,&nbsp;lf,&nbsp;lh,&nbsp;rf,&nbsp;rh,&nbsp;t",
   "metadata": {
    "tags": [],
    "cell_id": "00027-34935fad-ac74-4fe4-857c-0b76d7d23138",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "WM:&nbsp;&nbsp;&nbsp;&nbsp;0bk_body,&nbsp;0bk_faces,&nbsp;0bk_nir,&nbsp;0bk_placed,&nbsp;0bk_tools,&nbsp;&nbsp;2bk_body,&nbsp;2bk_faces,&nbsp;2bk_nir,&nbsp;2bk_placed,&nbsp;2bk_tools,&nbsp;0bk_cor,&nbsp;0bk_err,&nbsp;&nbsp;2bk_cor,&nbsp;2bk_err,&nbsp;&nbsp;all_bk_cor,&nbsp;all_bk_err",
   "metadata": {
    "tags": [],
    "cell_id": "00028-1fb60741-5370-4c54-9bd3-f100155f0be1",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "EMOTION:&nbsp;feat,&nbsp;neutral",
   "metadata": {
    "tags": [],
    "cell_id": "00029-c4e56c9a-bc09-472c-9b56-0630de621780",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00033-e9ca7329-da4c-4809-b9ef-d51101a1149b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "GAMBLING:&nbsp;loss,&nbsp;loss_event,&nbsp;win,&nbsp;win_event,&nbsp;neut_event",
   "metadata": {
    "tags": [],
    "cell_id": "00030-6ba83491-909a-4dfc-80a0-20d638d21250",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "LANGUAGE:&nbsp;cue, math,&nbsp;story, present_math,&nbsp;present_story,&nbsp;question_math,&nbsp;question_story,&nbsp;response_math,&nbsp;response_story",
   "metadata": {
    "tags": [],
    "cell_id": "00031-6b6decc4-7058-4caf-a16c-0a5ac2a4cf45",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "RELATIONAL:&nbsp;error,&nbsp;match,&nbsp;relation",
   "metadata": {
    "tags": [],
    "cell_id": "00032-9d385d85-c308-43b8-8a96-002791b220a1",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "SOCIAL:&nbsp;mental_resp,&nbsp;mental,&nbsp;other_resp,&nbsp;rnd",
   "metadata": {
    "tags": [],
    "cell_id": "00033-3f400221-919b-49c1-8d42-ab5d7ca043a4",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Task Data Retrieval",
   "metadata": {
    "tags": [],
    "cell_id": "00034-41f738af-13ad-429b-a92f-b45de714cb85",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00035-11e34364-07e4-4903-9dd6-8e91beb97b2d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "54539e0d",
    "execution_start": 1626705167088,
    "execution_millis": 533,
    "deepnote_cell_type": "code"
   },
   "source": "task = \"language\"\nconditions = [\"question_math\", \"response_math\"] # \"question_story\", \"response_story\"\nshift = 6\n\navg_qs = np.empty(shape=0)  #question story\navg_qm = np.empty(shape=0)  #question math\navg_rs = np.empty(shape=0)  #response story\navg_rm = np.empty(shape=0)  #response math\navgs = []\n\nfor subject in subjects:\n  print(subject)\n  # Get the average signal in each region for each condition\n\n  # format: condition, then LR/RL (direction), then dictionary w/ trials\n  # 4 trials for story, 9 trials for math\n  evs = [load_evs(subject, task, cond) for cond in conditions]\n  \n  # conditions, \n  avgs = [selective_average(timeseries_task[subject], ev) for ev in evs]\n  ## BOLD data timeseries: /V\\M\\----/\\/\\/\n  ## EV data:              -------~~|****|\n  ##                              ^ onset\n  ##                               **** = duration\n  ## skip = 2 = ~~\n\n  \n  # add subject average to avg arrays\n  #avg_qs = np.append(avg_qs, avgs[0])\n  #avg_qm = np.append(avg_qm, avgs[1])\n  #avg_rs = np.append(avg_rs, avgs[2])\n  #avg_rm = np.append(avg_rm, avgs[3])\n\n#print(len(avgs[2][1]))\n#print(evs[0][0]['onset'])\n#print(np.array(avgs).shape)\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "0\nframes shape: : (2,)\ndifferences: [  1   1   1   1   1  13   1   1   1   1   1  10   1   1   1   1   1 114\n   1   1   1   1   1  10   1   1   1   1  11   1   1   1   1  10   1   1\n   1   1  13   1   1   1   1   1  11   1   1   1   1  13   1   1   1   1\n   1  13   1   1   1   1]\nrun_frames after: : [ 46  47  48  49  50  51  64  65  66  67  68  69  79  80  81  82  83  84\n 198 199 200 201 202 203 213 214 215 216 217 228 229 230 231 232 242 243\n 244 245 246 259 260 261 262 263 264 275 276 277 278 279 292 293 294 295\n 296 297 310 311 312 313 314]\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_117/696234149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# conditions,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mavgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mselective_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m## BOLD data timeseries: /V\\M\\----/\\/\\/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m## EV data:              -------~~|****|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_117/696234149.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# conditions,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mavgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mselective_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m## BOLD data timeseries: /V\\M\\----/\\/\\/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m## EV data:              -------~~|****|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_117/1429640821.py\u001b[0m in \u001b[0;36mselective_average\u001b[0;34m(timeseries_data, ev, skip)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mselected_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_frames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;31m# Take the average in each parcel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00039-2b8707fe-e575-43ac-92f5-3e6cf97edaea",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a6a66dfd",
    "execution_start": 1626469592893,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": "print(len(avgs))\nprint(len(avgs))\nprint(avgs[0][0][8])\n#print(avgs)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "2\n2\n[-23.82484177 -51.85484177 -23.57484177 -13.85484177  22.27515823\n  19.37515823  -9.29484177  -6.25484177  -5.61484177 -12.68484177\n  -1.40484177   8.97515823  -1.45484177  11.94515823  19.00515823\n  33.67515823  34.87515823  33.76515823  -7.68484177 -18.99484177\n  -0.29484177  -9.29484177 -25.28484177 -45.98484177 -23.16484177\n -21.93484177 -17.94484177 -23.19484177 -26.36484177   0.74515823\n -11.22484177   3.25515823  -4.99484177  30.25515823  20.61515823\n  12.83515823  12.55515823  -5.12484177 -11.82484177 -28.93484177\n -18.00484177  25.49515823  -1.47484177   5.84515823  21.38515823\n   8.97515823 -13.12484177  17.77515823  24.78515823  52.68515823\n  30.74515823  -4.82484177  29.27515823  30.51515823  59.69515823\n  36.71515823  32.94515823]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00039-3a3559b7-27fd-41ad-a9ec-7181acfa7872",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "e2727631",
    "execution_start": 1626469592894,
    "execution_millis": 15,
    "deepnote_cell_type": "code"
   },
   "source": "print(avgs[0][0])\n#print(len(evs[3][1]['onset']))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "[[-12.84082278 -47.04082278   8.75917722 ...  58.75917722  77.55917722\n   76.75917722]\n [-23.12911392 -13.22911392 -45.92911392 ...  36.37088608  31.57088608\n  -22.82911392]\n [-88.4085443  -58.4085443  -29.4085443  ... -15.9085443  -10.3085443\n  -49.9085443 ]\n ...\n [ 17.62088608  47.62088608  50.42088608 ... -24.87911392  71.82088608\n   32.42088608]\n [ 29.86708861 -73.63291139 146.76708861 ...  81.66708861  65.06708861\n   94.66708861]\n [112.17689873  72.47689873  55.77689873 ... -28.12310127 -69.62310127\n  -63.92310127]]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00040-cb84d0c0-0985-4bc1-ab15-1876111112ce",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a2bc39e6",
    "execution_start": 1626469592895,
    "execution_millis": 14,
    "deepnote_cell_type": "code"
   },
   "source": "print(evs[0][0])",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'onset': array([ 34.05 ,  47.431,  58.693, 143.991, 154.307, 166.648, 182.189,\n       195.01 , 208.218, 221.746]), 'duration': array([3.675, 3.622, 3.625, 3.058, 3.155, 4.07 , 3.565, 3.716, 3.948,\n       3.605]), 'amplitude': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00041-60baa365-362e-4a01-bb75-178e451058ee",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b8b42cb5",
    "execution_start": 1626469592944,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "source": "print(len(evs))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00042-da937737-4de3-41e4-9a52-0a97aa80af47",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "execution_start": 1626469592945,
    "execution_millis": 149461,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0cfc2388-b22c-424d-b321-d09bf45f57a9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "c94308f2-8344-4770-9a29-afd426ed4526",
  "deepnote_execution_queue": []
 }
}