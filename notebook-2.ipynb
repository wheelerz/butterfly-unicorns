{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 2021 NMA Butterfly Unicorns Project Team",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00000-ece2a379-a93c-430e-8bad-f70f3440cf8c",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Anjali Srinivasan, Aaditya Prasad, Zac Wheeler",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00001-ecf5eeeb-df6f-4052-8884-c539960dbdfd",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a90608b6",
    "execution_start": 1626715217877,
    "execution_millis": 4838,
    "cell_id": "00002-4c50b91f-eda8-4241-8359-3a30a8f8b3c8",
    "deepnote_cell_type": "code"
   },
   "source": "# imports\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# for visualization\n!pip install nilearn --quiet\nfrom nilearn import plotting, datasets",
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n/root/venv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n  \"Numpy arrays.\", FutureWarning)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Retrieval",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00003-5c46d63a-2fa5-4ba1-ab57-bb7d87d68125",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f620f31e",
    "execution_start": 1626715222723,
    "execution_millis": 2,
    "cell_id": "00004-3d652d1a-5e06-4e52-b28b-aa846bbf2d51",
    "deepnote_cell_type": "code"
   },
   "source": "# The download cells will store the data in nested directories starting here:\nHCP_DIR = \"./hcp\"\nif not os.path.isdir(HCP_DIR):\n  os.mkdir(HCP_DIR)\n\n# The data shared for NMA projects is a subset of the full HCP dataset\nN_SUBJECTS = 339\n\n# The data have already been aggregated into ROIs from the Glasesr parcellation\nN_PARCELS = 360\n\n# The acquisition parameters for all tasks were identical\nTR = 0.72  # Time resolution, in sec\n\n# The parcels are matched across hemispheres with the same order\nHEMIS = [\"Right\", \"Left\"]\n\n# Each experiment was repeated multiple times in each subject\nN_RUNS_REST = 4\nN_RUNS_TASK = 2\n\n# Time series data are organized by experiment, with each experiment\n# having an LR and RL (phase-encode direction) acquistion\nBOLD_NAMES = [\n  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n]\n\n# You may want to limit the subjects used during code development.\n# This will use all subjects:\nsubjects = range(N_SUBJECTS)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Download",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00005-5aa9ee33-de0e-41f0-91f8-12dabcc9cf62",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "76c45edd",
    "execution_start": 1626715222735,
    "execution_millis": 1,
    "cell_id": "00006-3265d4a6-7f76-435e-92b7-facc17a92079",
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_task.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/s4h8j/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c2c5c0bd",
    "execution_start": 1626715222742,
    "execution_millis": 2,
    "cell_id": "00007-2a9c4f37-26fb-42f2-b39b-c5b29698aabe",
    "deepnote_cell_type": "code"
   },
   "source": "fname = \"hcp_covariates.tgz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/x5p4g/download/\n  !tar -xzf $fname -C $HCP_DIR --strip-components=1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "81d454e4",
    "execution_start": 1626715222751,
    "execution_millis": 34,
    "cell_id": "00008-dbf0e7f1-9e70-401f-9778-bc222dacf7c0",
    "deepnote_cell_type": "code"
   },
   "source": "fname = f\"{HCP_DIR}/atlas.npz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/j5kuc/download",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Region Info",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00009-917ccd98-508e-452b-a9eb-2f2bbd3b56e9",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d889d836",
    "execution_start": 1626715222786,
    "execution_millis": 0,
    "cell_id": "00010-16251437-34cb-468c-a8c8-e4b5016f1341",
    "deepnote_cell_type": "code"
   },
   "source": "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\nregion_info = dict(\n    name=regions[0].tolist(),\n    network=regions[1],\n    myelin=regions[2].astype(np.float),\n)\n\n\n#print(region_info)\n#print(len(region_info[\"name\"]))\n#print(len(region_info[\"network\"]))\n#print(len(region_info[\"myelin\"]))\n\n#print(region_info[\"name\"])\n#print(region_info[\"network\"])\n#print()\n\n#names = region_info[\"name\"]\n#networks = region_info[\"network\"]\n\n#language_areas = []\n\n#for i in range(len(networks)):\n#  if networks[i] == \"Auditory\":\n#    print(i, names[i])\n\n#posterior-mu = posterior multimodal",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "68c98641",
    "execution_start": 1626715222786,
    "execution_millis": 0,
    "cell_id": "00011-fca2a93c-5586-417b-a60e-c01e4794dad0",
    "deepnote_cell_type": "code"
   },
   "source": "with np.load(f\"{HCP_DIR}/atlas.npz\") as dobj:\n  atlas = dict(**dobj)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Helper Functions (For Data Loading)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00012-1e902dad-fb89-4095-b31c-3effa5fa69b6",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "get_image_ids(name)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00013-d20bf3bd-f621-45c2-8440-35c982b6ed68",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6be28f60",
    "execution_start": 1626715222787,
    "execution_millis": 31958,
    "cell_id": "00014-cb36300c-d47f-47ee-99a4-7b30cc816c92",
    "deepnote_cell_type": "code"
   },
   "source": "def get_image_ids(name):\n  \"\"\"Get the 1-based image indices for runs in a given experiment.\n\n    Args:\n      name (str) : Name of experiment (\"rest\" or name of task) to load\n    Returns:\n      run_ids (list of int) : Numeric ID for experiment image files\n\n  \"\"\"\n  run_ids = [\n    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n  ]\n  if not run_ids:\n    raise ValueError(f\"Found no data for '{name}''\")\n  return run_ids",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_timeseries(subject, name, runs=None, concat=True, remove_mean=True)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00015-fb8cdb44-58cb-463d-863c-3748420140f4",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2d7b457b",
    "execution_start": 1626715222814,
    "execution_millis": 0,
    "cell_id": "00016-78c4b7e6-e290-4136-8164-c43d94d03306",
    "deepnote_cell_type": "code"
   },
   "source": "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of experiment (\"rest\" or name of task) to load\n    run (None or int or list of ints): 0-based run(s) of the task to load,\n      or None to load all runs.\n    concat (bool) : If True, concatenate multiple runs in time\n    remove_mean (bool) : If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_tp array): Array of BOLD data values\n\n  \"\"\"\n  # Get the list relative 0-based index of runs to use\n  if runs is None:\n    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n  elif isinstance(runs, int):\n    runs = [runs]\n\n  # Get the first (1-based) run id for this experiment \n  offset = get_image_ids(name)[0]\n\n  # Load each run's data\n  bold_data = [\n      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n  ]\n\n  # Optionally concatenate in time\n  if concat:\n    bold_data = np.concatenate(bold_data, axis=-1)\n\n  return bold_data\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_single_timeseries(subject, bold_run, remove_mean=True)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00017-072db478-85d0-48f8-8757-51ea8217dc7b",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d3045ff1",
    "execution_start": 1626715222814,
    "execution_millis": 1,
    "cell_id": "00018-140ce4e4-03ed-41cd-985f-e0938c91b3f4",
    "deepnote_cell_type": "code"
   },
   "source": "def load_single_timeseries(subject, bold_run, remove_mean=True):\n  \"\"\"Load timeseries data for a single subject and single run.\n  \n  Args:\n    subject (int): 0-based subject ID to load\n    bold_run (int): 1-based run index, across all tasks\n    remove_mean (bool): If True, subtract the parcel-wise mean\n\n  Returns\n    ts (n_parcel x n_timepoint array): Array of BOLD data values\n\n  \"\"\"\n  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n  ts = np.load(f\"{bold_path}/{bold_file}\")\n  if remove_mean:\n    ts -= ts.mean(axis=1, keepdims=True)\n  return ts",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "load_evs(subject, name, condition)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00019-dc06899e-e205-46fc-ae89-29ca50d6ace0",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c0d0c9",
    "execution_start": 1626715222815,
    "execution_millis": 0,
    "cell_id": "00020-87db127b-5b4a-4ea5-a512-c8f534e3af7d",
    "deepnote_cell_type": "code"
   },
   "source": "def load_evs(subject, name, condition):\n  \"\"\"Load EV (explanatory variable) data for one task condition.\n\n  Args:\n    subject (int): 0-based subject ID to load\n    name (str) : Name of task\n    condition (str) : Name of condition\n\n  Returns\n    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n      of the condition for each run.\n\n  \"\"\"\n  evs = []\n  for id in get_image_ids(name):\n    task_key = BOLD_NAMES[id - 1]\n    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n    evs.append(ev)\n  return evs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "condition_frames(run_evs, skip=0)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00021-76c18b2d-a908-4bb4-8584-656ccb9cee73",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c217cd8d",
    "execution_start": 1626715222842,
    "execution_millis": 1,
    "cell_id": "00022-6fd848b1-c4a2-431c-8996-fdf78cbf0eff",
    "deepnote_cell_type": "code"
   },
   "source": "def get_frames_for_evs(run_evs, skip=0):\n  \"\"\"Identify timepoints corresponding to a given condition in each run.\n\n  Args:\n    run_evs (list of dicts) : Onset and duration of the event, per run\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:\n    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n\n  \"\"\"\n  frames_list = []\n  for ev in run_evs:\n    #print(\"ev: \", ev)\n\n    # Determine when trial starts, rounded down\n    start = np.floor(ev[\"onset\"] / TR).astype(int)\n\n    # Use trial duration to determine how many frames to include for trial\n    # TR = 0.72  # Time resolution, in sec\n    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n\n    # Take the range of frames that correspond to this specific trial\n    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n\n    frames_list.append(frames)\n\n  return frames_list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "selective_average(timeseries_data, ev, skip=0)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00023-bb55abe7-2b1a-43e6-9d3f-add92e9866a9",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d9d68b",
    "execution_start": 1626715222843,
    "execution_millis": 0,
    "cell_id": "00024-143722ab-5227-46b9-bc01-928f6fff4cd0",
    "deepnote_cell_type": "code"
   },
   "source": "def selective_averages(timeseries_data, ev, skip=0):\n  \"\"\"Take the temporal mean across frames for a given condition.\n\n  Args:\n    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n    ev (dict or list of dicts): Condition timing information\n    skip (int) : Ignore this many frames at the start of each trial, to account\n      for hemodynamic lag\n\n  Returns:(\n    selected_data: has shape: 2, 360, [number of trial runs] and contains the \n    average for each trial run\n\n  \"\"\"\n  # Ensure that we have lists of the same length\n  if not isinstance(timeseries_data, list):\n    timeseries_data = [timeseries_data]\n  if not isinstance(ev, list):\n    ev = [ev]\n  if len(timeseries_data) != len(ev):\n    raise ValueError(\"Number of `timeseries_data` and `ev` objects given must match.\")\n\n  # Identify the indices of relevant frames\n  frames_list = get_frames_for_evs(ev, skip)\n  print(\"frames_list[0]: \", end=\": \")\n  print(frames_list[0])\n  print(\"frames_list[1]: \", end=\": \")\n  print(frames_list[1])\n  #print(len(frames_list[0]))\n\n  # Select the frames from each image\n  # timeseries_data has shape: 2, 360, 316 (we already indexed by subject before passing it in)\n  # selected_data has shape: 2, 360, [number of trial runs] and contains the average for each trial run\n  selected_data = np.empty((2,360,), dtype=object)\n  selected_data.fill([])\n  \n  \n  # RL, then LR\n  # each measurement direction\n  for rl in range(2):\n    # each brain parcel\n    for parcel in range(360):\n      run_count = 0\n      # each trial run\n      for run in frames_list[rl]:\n        window_sum = 0\n        # each BOLD signal measurement in the trial run\n        for measurement_index in run:\n          window_sum = window_sum + timeseries_data[rl][parcel][measurement_index]\n        selected_data[rl][parcel] = np.append(selected_data[rl][parcel], window_sum/len(run))\n        run_count = run_count + 1\n      #print(\"parcel:\", parcel, selected_data[rl][parcel])\n  \n  \n  return selected_data",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Task Analysis",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00025-59af88e3-e5ca-45ec-af29-06cd07bd545f",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### timeseries_task",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00026-3fe916d2-8929-44eb-9c62-025b8548750a",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "755868b1",
    "execution_start": 1626715222844,
    "execution_millis": 827,
    "cell_id": "00027-39758691-8b6f-42c5-b2f7-f7160f189938",
    "deepnote_cell_type": "code"
   },
   "source": "timeseries_task = []\n\n# timeseries_task format: subject, then LR/RL, then parcel, then time\n# 339, 2, 360, 316\nfor subject in subjects:\n  timeseries_task.append(load_timeseries(subject, \"language\", concat=False))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f944442a",
    "execution_start": 1626715223678,
    "execution_millis": 1,
    "cell_id": "00028-f5c9ff00-3000-4040-a4d3-f01c11255d27",
    "deepnote_cell_type": "code"
   },
   "source": "#print(np.array(timeseries_task).shape)\n#print(timeseries_task[0])\n#print()\n#print(timeseries_task[0][0][0][0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Task Descriptions",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00029-993a5f19-be66-4cde-91b8-f884eac06f38",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "MOTOR:&nbsp;cue,&nbsp;lf,&nbsp;lh,&nbsp;rf,&nbsp;rh,&nbsp;t",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00030-18f7642e-c6b5-42b1-b49d-06a828428ae8",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "WM:&nbsp;&nbsp;&nbsp;&nbsp;0bk_body,&nbsp;0bk_faces,&nbsp;0bk_nir,&nbsp;0bk_placed,&nbsp;0bk_tools,&nbsp;&nbsp;2bk_body,&nbsp;2bk_faces,&nbsp;2bk_nir,&nbsp;2bk_placed,&nbsp;2bk_tools,&nbsp;0bk_cor,&nbsp;0bk_err,&nbsp;&nbsp;2bk_cor,&nbsp;2bk_err,&nbsp;&nbsp;all_bk_cor,&nbsp;all_bk_err",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00031-4053bae9-4761-49f4-8d97-da2199c070d9",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "EMOTION:&nbsp;feat,&nbsp;neutral",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00032-0b98f5b2-1903-4872-98cd-4a2cd0375a3b",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "GAMBLING:&nbsp;loss,&nbsp;loss_event,&nbsp;win,&nbsp;win_event,&nbsp;neut_event",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00033-2cca6b18-7462-41b3-8602-5326184c016a",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "LANGUAGE:&nbsp;cue, math,&nbsp;story, present_math,&nbsp;present_story,&nbsp;question_math,&nbsp;question_story,&nbsp;response_math,&nbsp;response_story",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00034-1ceaa84e-b50c-4cfc-94b6-c1032ef8653d",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "RELATIONAL:&nbsp;error,&nbsp;match,&nbsp;relation",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00035-e53efaab-5da6-441b-b568-320a6481403f",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "SOCIAL:&nbsp;mental_resp,&nbsp;mental,&nbsp;other_resp,&nbsp;rnd",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00036-2bd5c7d8-8d20-4dec-bf67-9d4c30682749",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Task Data Retrieval",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00037-c03380f8-cd5b-473f-a6c1-9b0a87c41762",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f9f1249b",
    "execution_start": 1626715223692,
    "execution_millis": 184,
    "cell_id": "00038-d2815d8a-ca28-49d7-9922-772e8367b5f9",
    "deepnote_cell_type": "code"
   },
   "source": "def get_region_avgs_for_cond(subject, task, cond):\n  avgs = []\n  # get a list of dicts describing the active time windows based on the subject and task/condition\n  evs = load_evs(subject, task, cond)\n  # get array of the average BOLD data w/ an avg for each trial, \n  # in each region, and one for each of RL/LR\n  avgs = selective_averages(timeseries_task[subject], evs)\n  return avgs\n\ntask = \"language\"\nsubject = 5\nshift = 6\n\navgs_qm = []  #question math\navgs_qs = []  #question story\navgs_all = []\n\navgs_qm = get_region_avgs_for_cond(subject, task, \"question_math\")\n#avgs_qs = get_region_avgs_for_cond(subject, task, \"question_story\")\n\nprint(np.array(avgs_qm).shape)\nprint(\"question math: \")\n#print(avgs_qm)\nprint(avgs_qm[0][0])\nprint(avgs_qm[1][0])\n#avgs_all = np.append(avgs_qm[0], avgs_qm[1])\n#print(avgs_all.shape)\n\n#print(np.array(avgs_qs).shape)\n#print(\"question story: \")\n#print(avgs_qs)\n\n'''\nfor subject in subjects:\n  print(subject)\n  # Get the average signal in each region for each condition\n\n  # format: condition, then LR/RL (direction), then dictionary w/ trials\n  # 4 trials for story, 9 trials for math\n  evs = [load_evs(subject, task, cond) for cond in conditions]\n  \n  # conditions, \n  avgs = [selective_average(timeseries_task[subject], ev) for ev in evs]\n  ## BOLD data timeseries: /V\\M\\----/\\/\\/\n  ## EV data:              -------~~|****|\n  ##                              ^ onset\n  ##                               **** = duration\n  ## skip = 2 = ~~\n\n  \n  # add subject average to avg arrays\n  #avg_qs = np.append(avg_qs, avgs[0])\n  #avg_qm = np.append(avg_qm, avgs[1])\n  #avg_rs = np.append(avg_rs, avgs[2])\n  #avg_rm = np.append(avg_rm, avgs[3])\n\n#print(len(avgs[2][1]))\n#print(evs[0][0]['onset'])\n#print(np.array(avgs).shape)\n'''",
   "outputs": [
    {
     "name": "stdout",
     "text": "frames_list[0]: : [array([46, 47, 48, 49, 50]), array([64, 65, 66, 67, 68, 69]), array([81, 82, 83, 84, 85, 86]), array([202, 203, 204, 205, 206]), array([219, 220, 221, 222, 223]), array([238, 239, 240, 241, 242]), array([256, 257, 258, 259, 260, 261]), array([274, 275, 276, 277, 278]), array([292, 293, 294, 295, 296, 297]), array([310, 311, 312, 313, 314])]\nframes_list[1]: : [array([50, 51, 52, 53, 54]), array([68, 69, 70, 71, 72]), array([128, 129, 130, 131, 132]), array([144, 145, 146, 147, 148, 149]), array([203, 204, 205, 206, 207, 208]), array([221, 222, 223, 224, 225]), array([240, 241, 242, 243, 244, 245]), array([305, 306, 307, 308, 309])]\n(2, 360)\nquestion math: \n[ 31.26867089  78.56867089  19.58533755  28.92867089  -9.07132911\n -13.31132911 -13.94799578 -10.87132911 -52.34799578 -92.93132911]\n[ 56.50139241  -1.71860759  18.20139241  58.57805907  36.34472574\n  -4.97860759  42.64472574 -36.73860759]\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 16,
     "data": {
      "text/plain": "\"\\nfor subject in subjects:\\n  print(subject)\\n  # Get the average signal in each region for each condition\\n\\n  # format: condition, then LR/RL (direction), then dictionary w/ trials\\n  # 4 trials for story, 9 trials for math\\n  evs = [load_evs(subject, task, cond) for cond in conditions]\\n  \\n  # conditions, \\n  avgs = [selective_average(timeseries_task[subject], ev) for ev in evs]\\n  ## BOLD data timeseries: /V\\\\M\\\\----/\\\\/\\\\/\\n  ## EV data:              -------~~|****|\\n  ##                              ^ onset\\n  ##                               **** = duration\\n  ## skip = 2 = ~~\\n\\n  \\n  # add subject average to avg arrays\\n  #avg_qs = np.append(avg_qs, avgs[0])\\n  #avg_qm = np.append(avg_qm, avgs[1])\\n  #avg_rs = np.append(avg_rs, avgs[2])\\n  #avg_rm = np.append(avg_rm, avgs[3])\\n\\n#print(len(avgs[2][1]))\\n#print(evs[0][0]['onset'])\\n#print(np.array(avgs).shape)\\n\""
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "avgs_qm_reshaped = np.moveaxis(avgs_qm, 0, 1)\n#print(avgs_qm_reshaped.shape)\n#print(list(avgs_qm_reshaped[0][0]))\navgs_qm_new = [None]*360\nprint(len(list(avgs_qm_reshaped[0][0])))\nprint(\"+\")\nprint(len(list(avgs_qm_reshaped[0][1])))\nprint(\"=\")\nconcat_list = list(avgs_qm_reshaped[0][0]) + list(avgs_qm_reshaped[0][1])\nprint(len(concat_list))\n\nfor i in range(360):\n    avgs_qm_new[i] = list(avgs_qm_reshaped[i][0]) + list(avgs_qm_reshaped[i][1])\n\n#print(avgs_qm_new)",
   "metadata": {
    "tags": [],
    "cell_id": "00039-84e3dca4-0a03-445e-8ac8-43f4edbb7410",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ffa3b484",
    "execution_start": 1626715223888,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "10\n+\n8\n=\n18\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0cfc2388-b22c-424d-b321-d09bf45f57a9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "046d7d7a-a47e-41d6-8336-08e56ac4bf56",
  "deepnote_execution_queue": []
 }
}